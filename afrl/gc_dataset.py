import dataclasses
from functools import partial
from typing import List, Dict

import numpy as np
import jax
import jax.numpy as jnp
from flax.core.frozen_dict import FrozenDict
from flax.core import freeze
import ml_collections

from jaxrl_m.dataset import Dataset, ReplayBuffer


@partial(jax.jit, static_argnames=('padding',))
def random_crop(img, crop_from, padding):
    padded_img = jnp.pad(img, ((padding, padding), (padding, padding), (0, 0)), mode='edge')
    return jax.lax.dynamic_slice(padded_img, crop_from, img.shape)


@partial(jax.jit, static_argnames=('padding',))
def batched_random_crop(imgs, crop_froms, padding):
    return jax.vmap(random_crop, (0, 0, None))(imgs, crop_froms, padding)


@dataclasses.dataclass
class GCDataset:
    dataset: Dataset
    p_randomgoal: float
    p_trajgoal: float
    p_currgoal: float
    geom_sample: int
    discount: float
    terminal_key: str = 'dones_float'
    reward_scale: float = 1.0
    reward_shift: float = -1.0
    terminal: bool = True

    @staticmethod
    def get_default_config():
        return ml_collections.ConfigDict({
            'p_randomgoal': 0.3,
            'p_trajgoal': 0.5,
            'p_currgoal': 0.2,
            'geom_sample': 0,
            'reward_scale': 1.0,
            'reward_shift': -1.0,
            'terminal': True,
        })

    def __post_init__(self):
        self.terminal_locs, = np.nonzero(self.dataset[self.terminal_key] > 0)
        assert np.isclose(self.p_randomgoal + self.p_trajgoal + self.p_currgoal, 1.0)

    def sample_goals(self, indx, p_randomgoal=None, p_trajgoal=None, p_currgoal=None):
        """Sample a final goal for each step in indx.

        With probability p_randomgoal, sample a random goal from the whole dataset.
        With probability p_trajgoal, sample a goal from the same trajectory (a random goal between the current step and the final state).
        With probability p_currgoal, return the same step.
        """
        if p_randomgoal is None:
            p_randomgoal = self.p_randomgoal
        if p_trajgoal is None:
            p_trajgoal = self.p_trajgoal
        if p_currgoal is None:
            p_currgoal = self.p_currgoal

        batch_size = len(indx)
        # Random goals
        goal_indx = np.random.randint(self.dataset.size, size=batch_size)
        
        # Goals from the same trajectory
        positions = np.searchsorted(self.terminal_locs, indx)
        positions[positions == len(self.terminal_locs)] -= 1    # In online setting, it is possible the last state is not terminal. So we directly set it to the last state.
        final_state_indx = self.terminal_locs[positions]

        distance = np.random.rand(batch_size)
        if self.geom_sample:
            us = np.random.rand(batch_size)
            middle_goal_indx = np.minimum(indx + np.ceil(np.log(1 - us) / np.log(self.discount)).astype(int), final_state_indx)
        else:
            middle_goal_indx = np.round((np.minimum(indx + 1, final_state_indx) * distance + final_state_indx * (1 - distance))).astype(int)

        goal_indx = np.where(np.random.rand(batch_size) < p_trajgoal / (1.0 - p_currgoal), middle_goal_indx, goal_indx)
        
        # Goals at the current state
        goal_indx = np.where(np.random.rand(batch_size) < p_currgoal, indx, goal_indx)
        return goal_indx

    def sample(self, batch_size: int, indx=None):
        if indx is None:
            indx = np.random.randint(self.dataset.size-1, size=batch_size)
        
        batch = self.dataset.sample(batch_size, indx)
        goal_indx = self.sample_goals(indx)

        success = (indx == goal_indx)
        batch['rewards'] = success.astype(float) * self.reward_scale + self.reward_shift
        if self.terminal:
            batch['masks'] = (1.0 - success.astype(float))
        else:
            batch['masks'] = np.ones(batch_size)
        batch['goals'] = jax.tree_map(lambda arr: arr[goal_indx], self.dataset['observations'])

        return batch

    @property
    def size(self):
        return self.dataset.size


@dataclasses.dataclass
class GCSDataset(GCDataset):
    way_steps: int = None
    high_p_randomgoal: float = 0.
    p_aug: float = None

    @staticmethod
    def get_default_config():
        return ml_collections.ConfigDict({
            'p_randomgoal': 0.3,
            'p_trajgoal': 0.5,
            'p_currgoal': 0.2,
            'geom_sample': 0,
            'reward_scale': 1.0,
            'reward_shift': 0.0,
            'terminal': False,
        })

    def sample(self, batch_size: int, indx=None):
        """
        batch['goals']: final goal, generated by self.sample_goals
        batch['low_goals']: select the states way_steps away from the current state as low goals
        batch['high_goals'] & batch['high_targets']: with probability high_p_randomgoal, sample a random state
            from the whole dataset as high goal, and use the way_steps away step (but before trajectory ends) as high target.
            Otherwise, sample a state between the current step and the final step (final_state_indx) as high goal,
            and use the way_steps away step (but before high goal) as high target.
        """
        if indx is None:
            indx = np.random.randint(self.dataset.size-1, size=batch_size)

        batch = self.dataset.sample(batch_size, indx)
        goal_indx = self.sample_goals(indx)

        success = (indx == goal_indx)

        batch['rewards'] = success.astype(float) * self.reward_scale + self.reward_shift

        if self.terminal:
            batch['masks'] = (1.0 - success.astype(float))
        else:
            batch['masks'] = np.ones(batch_size)

        batch['goals'] = jax.tree_map(lambda arr: arr[goal_indx], self.dataset['observations'])

        positions = np.searchsorted(self.terminal_locs, indx)
        positions[positions == len(self.terminal_locs)] -= 1    # In online setting, it is possible the last state is not terminal. So we directly set it to the last state.
        final_state_indx = self.terminal_locs[positions]
        way_indx = np.minimum(indx + self.way_steps, final_state_indx)
        batch['low_goals'] = jax.tree_map(lambda arr: arr[way_indx], self.dataset['observations'])

        distance = np.random.rand(batch_size)

        high_traj_goal_indx = np.round((np.minimum(indx + 1, final_state_indx) * distance + final_state_indx * (1 - distance))).astype(int)
        high_traj_target_indx = np.minimum(indx + self.way_steps, high_traj_goal_indx)
        fast_high_traj_traget_indx = np.minimum(indx + 1, high_traj_goal_indx)

        high_random_goal_indx = np.random.randint(self.dataset.size, size=batch_size)
        high_random_target_indx = np.minimum(indx + self.way_steps, final_state_indx)
        fast_high_random_target_indx = np.minimum(indx + 1, final_state_indx)

        pick_random = (np.random.rand(batch_size) < self.high_p_randomgoal)
        high_goal_idx = np.where(pick_random, high_random_goal_indx, high_traj_goal_indx)
        high_target_idx = np.where(pick_random, high_random_target_indx, high_traj_target_indx)
        fast_high_target_idx = np.where(pick_random, fast_high_random_target_indx, fast_high_traj_traget_indx)

        batch['high_goals'] = jax.tree_map(lambda arr: arr[high_goal_idx], self.dataset['observations'])
        batch['high_targets'] = jax.tree_map(lambda arr: arr[high_target_idx], self.dataset['observations'])
        batch['fast_high_targets'] = jax.tree_map(lambda arr: arr[fast_high_target_idx], self.dataset['observations'])

        if isinstance(batch['goals'], FrozenDict):
            # Freeze the other observations
            batch['observations'] = freeze(batch['observations'])
            batch['next_observations'] = freeze(batch['next_observations'])
        
        if self.p_aug is not None:
            if np.random.rand() < self.p_aug:
                aug_keys = ['observations', 'next_observations', 'goals', 'low_goals', 'high_goals', 'high_targets', 'fast_high_targets']
                padding = 4
                crop_froms = np.random.randint(0, 2 * padding + 1, (batch_size, 2))
                crop_froms = np.concatenate([crop_froms, np.zeros((batch_size, 1), dtype=np.int32)], axis=1)
                for key in aug_keys:
                    batch[key] = jax.tree_map(lambda arr: np.array(batched_random_crop(arr, crop_froms, padding)) if len(arr.shape) == 4 else arr, batch[key])

        return batch


@dataclasses.dataclass
class GCSReplayBuffer(GCSDataset):
    dataset: ReplayBuffer
    sample_scheme: str = 'invalid'
    relabelling_prob: float = 0.8

    def add_trajectory(self, traj: List[Dict]):
        for transition in traj:
            self.dataset.add_transition(transition)
        self.terminal_locs, = np.nonzero(self.dataset[self.terminal_key] > 0)
    
    def sample(self, batch_size: int, indx=None, way_steps: int = None):
        if self.sample_scheme == 'invalid':
            raise ValueError('sample_scheme not set')
        elif self.sample_scheme == 'original':
            return super().sample(batch_size, indx)
        elif self.sample_scheme == 'her':
            # My new sampling scheme
            if way_steps is None:
                way_steps = self.way_steps

            if indx is None:
                indx = np.random.randint(self.dataset.size-1, size=batch_size)
            
            # goal
            batch = self.dataset.sample(batch_size, indx)
            goal_indx = self.sample_goals(indx)

            success = (indx == goal_indx)
            batch['rewards'] = success.astype(float) * self.reward_scale + self.reward_shift
            if self.terminal:
                batch['masks'] = (1.0 - success.astype(float))
            else:
                batch['masks'] = np.ones(batch_size)
            batch['goals'] = jax.tree_map(lambda arr: arr[goal_indx], self.dataset['observations'])

            # low goal
            positions = np.searchsorted(self.terminal_locs, indx)
            positions[positions == len(self.terminal_locs)] -= 1    # In online setting, it is possible the last state is not terminal. So we directly set it to the last state.
            final_state_indx = self.terminal_locs[positions]
            way_indx = np.minimum(indx + way_steps, final_state_indx)
            
            # with probability relabelling_prob, sample a low goal between the current state and the way_indx
            distance = np.random.rand(batch_size)
            low_goal_1_indx = np.round((np.minimum(indx + 1, way_indx) * distance + way_indx * (1 - distance))).astype(int)
            low_goal_1 = jax.tree_map(lambda arr: arr[low_goal_1_indx], self.dataset['observations'])

            low_goal_2 = jax.tree_map(lambda arr: arr[indx], self.dataset['traj_subgoals'])

            pick_random = np.expand_dims(np.random.rand(batch_size) < self.relabelling_prob, axis=1)

            batch['low_goals'] = np.where(pick_random, low_goal_1, low_goal_2)

            # do not produce high goal & high target (varying way_steps)
            return batch
        elif self.sample_scheme == 'plain':
            if indx is None:
                indx = np.random.randint(self.dataset.size-1, size=batch_size)
            batch = self.dataset.sample(batch_size, indx)
            return batch
        elif self.sample_scheme == 'herV2':
            if way_steps is None:
                way_steps = self.way_steps

            if indx is None:
                indx = np.random.randint(self.dataset.size-1, size=batch_size)
            batch = self.dataset.sample(batch_size, indx)

            positions = np.searchsorted(self.terminal_locs, indx)
            positions[positions == len(self.terminal_locs)] -= 1    # In online setting, it is possible the last state is not terminal. So we directly set it to the last state.
            final_state_indx = self.terminal_locs[positions]
            way_indx = np.minimum(indx + way_steps, final_state_indx)

            # with probability relabelling_prob, sample a low goal between the current state and the way_indx
            distance = np.random.rand(batch_size)
            low_goal_1_indx = np.round((np.minimum(indx + 1, way_indx) * distance + way_indx * (1 - distance))).astype(int)

            batch['masks'] = (1.0 - (indx == low_goal_1_indx).astype(float))

            low_goal_1 = jax.tree_map(lambda arr: arr[low_goal_1_indx], self.dataset['observations'])

            low_goal_2 = jax.tree_map(lambda arr: arr[indx], self.dataset['traj_subgoals'])

            pick_random = np.expand_dims(np.random.rand(batch_size) < self.relabelling_prob, axis=1)
            
            batch['traj_subgoals'] = np.where(pick_random, low_goal_1, low_goal_2)
            pre_distance = np.linalg.norm(batch['observations'][:, :2] - batch['traj_subgoals'][:, :2], axis=-1)
            post_distance = np.linalg.norm(batch['next_observations'][:, :2] - batch['traj_subgoals'][:, :2], axis=-1)
            batch['guided_rewards'] = pre_distance - post_distance
            batch['intrinsic_rewards'] = -post_distance
            return batch
        else:
            raise ValueError(f"illegal ample scheme: {self.sample_scheme}")
